# Convolutional Neural Networks woth Kera and Tesorflow

# Load packages
library(keras)
library(EBImage)

# Read Images
setwd('/Users/irinamahmudjanova/Documents/STUDY/DATA_SCIENCE/R_Udemy_ML /R-Course-HTML-Notes/R-for-Data-Science-and-Machine-Learning/Machine Learning with R/images')
pic1 <- c('p1.jpg', 'p2.jpg', 'p3.jpg', 'p4.jpg', 'p5.jpg',
          'c1.jpg', 'c2.jpg', 'c3.jpg', 'c4.jpg', 'c5.jpg',
          'b1.jpg', 'b2.jpg', 'b3.jpg', 'b4.jpg', 'b5.jpg')
train <- list()
for (i in 1:15) {train[[i]] <- readImage(pic1[i])}

pic2 <- c('p6.jpg', 'c6.jpg', 'b6.jpg')
test <- list()
for (i in 1:3) {test[[i]] <- readImage(pic2[i])}

# Explore
print(train[[12]])
summary(train[[12]])
display(train[[12]])
plot(train[[12]])

par(mfrow = c(3, 5))
for (i in 1:15) plot(train[[i]])
par(mfrow = c(1,1))

# Resize & combine
str(train)
for (i in 1:15) {train[[i]] <- resize(train[[i]], 100, 100)}
for (i in 1:3) {test[[i]] <- resize(test[[i]], 100, 100)}

train <- combine(train)
x <- tile(train, 5)
display(x, title='Pictures')

test <- combine(test)
y <- tile(test, 3)
display(y, title = 'Pics')

# Reorder dimension
train <- aperm(train, c(4, 1, 2, 3))
test <- aperm(test, c(4, 1, 2, 3))
str(train)

# Response
trainy <- c(0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2)
testy <- c(0, 1, 2)

# One hot encoding
trainLabels <- to_categorical(trainy)
testLabels <- to_categorical(testy)

# Model
model <- keras_model_sequential()

model %>%
         layer_conv_2d(filters = 32, 
                       kernel_size = c(3,3),
                       activation = 'relu',
                       input_shape = c(100, 100, 3)) %>%
         layer_conv_2d(filters = 32,
                       kernel_size = c(3,3),
                       activation = 'relu') %>%
         layer_max_pooling_2d(pool_size = c(2,2)) %>%
         layer_dropout(rate = 0.25) %>%
         layer_conv_2d(filters = 64,
                       kernel_size = c(3,3),
                       activation = 'relu') %>%
         layer_conv_2d(filters = 64,
                       kernel_size = c(3,3),
                       activation = 'relu') %>%
  
         layer_max_pooling_2d(pool_size = c(2,2)) %>%
         layer_dropout(rate = 0.25) %>%
         layer_flatten() %>%
         layer_dense(units = 256, activation = 'relu') %>%
         layer_dropout(rate=0.25) %>%
         layer_dense(units = 3, activation = 'softmax') %>%
         
         compile(loss = 'categorical_crossentropy',
                 optimizer = optimizer_sgd(lr = 0.01,
                                           decay = 1e-6,
                                           momentum = 0.9,
                                           nesterov = T),
                 metrics = c('accuracy'))
summary(model)

# Fit model
history <- model %>% fit(train,
             trainLabels,
             epochs = 60,
             batch_size = 32,
             validation_split = 0.2,
             validation_data = list(test, testLabels))

# Graph name fitMode2.png  at /Users/irinamahmudjanova/Documents/STUDY/DATA_SCIENCE/R_Udemy_ML /R-Course-HTML-Notes/R-for-Data-Science-and-Machine-Learning/Machine Learning with R/images
plot(history)
# Grapg Loss_accmodel2.png the same location

# Evaluation & Prediction - train data
model %>% evaluate(train, trainLabels)
pred <- model %>% predict_classes(train)
table(Predicted = pred, Actual = trainy)
#           Actual
# Predicted 0 1 2
# 0 5 0 0
# 1 0 5 0
# 2 0 0 5

prob <- model %>% predict_proba(train)
cbind(prob, Predicted_class = pred, Actual = trainy)
# Predicted_class Actual
# [1,] 7.334944e-01 2.462725e-05 2.664810e-01               0      0
# [2,] 6.663837e-01 1.442547e-05 3.336018e-01               0      0
# [3,] 7.772540e-01 1.486593e-05 2.227311e-01               0      0
# [4,] 7.781149e-01 3.936823e-03 2.179483e-01               0      0
# [5,] 6.663726e-01 1.053372e-03 3.325741e-01               0      0
# [6,] 1.621543e-05 9.999546e-01 2.923993e-05               1      1
# [7,] 5.423932e-06 9.999804e-01 1.400709e-05               1      1
# [8,] 2.457692e-04 9.992587e-01 4.955847e-04               1      1
# [9,] 1.127340e-05 9.999628e-01 2.590945e-05               1      1
# [10,] 7.839730e-03 9.852003e-01 6.959988e-03               1      1
# [11,] 2.904728e-01 6.140927e-05 7.094658e-01               2      2
# [12,] 4.217414e-01 2.928585e-05 5.782293e-01               2      2
# [13,] 2.711424e-01 2.918675e-05 7.288284e-01               2      2
# [14,] 3.646650e-01 1.544820e-05 6.353195e-01               2      2
# [15,] 2.022599e-01 1.043091e-04 7.976358e-01               2      2

# Evaluation & Prediction - test data
model %>% evaluate(test, testLabels)
# 3/3 [==============================] - 0s 12ms/step
# $loss
# [1] 0.2903638
# 
# $acc
# [1] 1

pred <- model %>% predict_classes(test)
table(Predicted = pred, Actual = testy)
#          Actual
#Predicted 0 1 2
#        0 1 0 0
#        1 0 1 0
#        2 0 0 1
prob <- model %>% predict_proba(test)
cbind(prob, Predicted_class = pred, Actual = testy)
#                                              Predicted_class Actual
# [1,] 0.6397491097 1.414011e-05 0.360236764               0      0
# [2,] 0.0009787297 9.969999e-01 0.002021314               1      1
# [3,] 0.3438147902 6.275594e-05 0.656122446               2      2
